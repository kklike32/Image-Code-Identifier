{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Handwriting Model Setup #\n",
        "This part revolves around defining utility functions for the entire pipeline for handwriting analysis."
      ],
      "metadata": {
        "id": "g212cIuwDN-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries ##\n",
        "Import the necessary training data from EMNIST to transfer, the tensorflow framework + auxiliary functionality to train a model, and numpy for basic array manipulation."
      ],
      "metadata": {
        "id": "jGJEpBipDZeo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "naG4_UDjlpdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b8a92f0-04b9-4d79-c54d-fe5eeaf1f827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: emnist in /usr/local/lib/python3.10/dist-packages (0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from emnist) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from emnist) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from emnist) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (3.4)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "    This file defines the model to be used for training, transferring learning, and then running the model \n",
        "    on student handwriting results.\n",
        "\"\"\"\n",
        "# dependencies #\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install emnist\n",
        "\n",
        "import numpy as np                                                              # array manipulation for weights\n",
        "import csv                                                                      # saving and loading weights\n",
        "import tensorflow as tf                                                         # model deployment\n",
        "import keras                                                                    # model deployment\n",
        "from tensorflow.keras.models import Sequential                                  # model initialization\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense        # model initialization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator             # student handwriting training\n",
        "import emnist                                                                   # transfer learning data\n",
        "from sklearn.model_selection import train_test_split                            # student handwriting training\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    GLOBAL VARIABLES\n",
        "\"\"\"\n",
        "special_chars = \"#%^&*()_-+={}[]\\\\<>,.?/\"\n",
        "num_chars = 26 + 26 + 10 + len(special_chars) # upper, lower, digits, special_chars"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Engineering ##\n",
        "This part of the code will load MNIST data, engineer it to optimize training, then load our new data and engineer it to fit the specifications of the MNIST data as best as we can."
      ],
      "metadata": {
        "id": "62Wok_apl3Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def shape_data(x_train, y_train, x_test, y_test):\n",
        "#     # shape\n",
        "#     x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "#     x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "\n",
        "#     # split\n",
        "#     y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_chars)\n",
        "#     y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_chars)\n",
        "# \n",
        "# \n",
        "def load_emnist():\n",
        "    # load mnist for transfer learning\n",
        "    print(\"loading data for transfer learning...\")\n",
        "    x_train, y_train = emnist.extract_training_samples('balanced')\n",
        "    x_test, y_test = emnist.extract_test_samples('balanced')\n",
        "\n",
        "        # # process data for optimal character recognition\n",
        "        # print(\"processing data for optimal results...\")\n",
        "        # x_train, y_train, x_test, y_test = shape_data(x_train, y_train, x_test, y_test)\n",
        "\n",
        "    return [x_train, x_test, y_train, y_test]\n",
        "\n",
        "\n",
        "def load_convex_data():\n",
        "    # load images\n",
        "    print(\"loading images for student handwriting...\")\n",
        "    dataset_dir = './student_handwriting/'\n",
        "    datagen = ImageDataGenerator(rescale=1.0/255.0)  # normalize pixel values between 0 and 1\n",
        "\n",
        "    # load the dataset using data generator\n",
        "    train_generator = datagen.flow_from_directory(\n",
        "        dataset_dir,\n",
        "        target_size=(28, 28),\n",
        "        color_mode='grayscale',\n",
        "        batch_size=32,\n",
        "        class_mode='sparse',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # load x & y data\n",
        "    x_data = train_generator[0][0]\n",
        "    y_data = train_generator[0][1]\n",
        "\n",
        "    # split data\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=17)\n",
        "\n",
        "    return [x_train, x_test, y_train, y_test]"
      ],
      "metadata": {
        "id": "jYxDV2NxltT5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment ##\n",
        "This part of the code will train the model on the MNIST data, save the resulting weights for easy initialization in the future, then transfer that learning to the new data for retraining. Throughout the process, weights will be sequentially saved in order to preserve progress and record evolution (for future optimization)."
      ],
      "metadata": {
        "id": "k0vl-QSYCyun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_architecture():\n",
        "    # define model architecture\n",
        "    print(\"defining model architecture...\")\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # compile model\n",
        "    print(\"compiling model...\")\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # return model\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_emnist(model, emnist_data):\n",
        "    # load data\n",
        "    x_train = emnist_data[0]\n",
        "    y_train = emnist_data[1]\n",
        "    x_test = emnist_data[2]\n",
        "    y_test = emnist_data[3]\n",
        "\n",
        "    # train model on MNIST\n",
        "    print(\"training on MNIST data...\")\n",
        "    model.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "    # save weights\n",
        "    save_weights(model)\n",
        "\n",
        "\n",
        "def save_weights(model):\n",
        "    # save weights\n",
        "    print(\"saving MNIST weights...\")\n",
        "    weights = model.get_weights()\n",
        "    with open(\"mnist_weights.csv\", 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        for weight in weights:\n",
        "            writer.writerow(weight.flatten())\n",
        "        \n",
        "\n",
        "def load_weights(filepath):\n",
        "    # load from file\n",
        "    print(\"loading weights for transfer learning...\")\n",
        "    with open(filepath, 'r') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        weights = []\n",
        "        for row in reader:\n",
        "            weights.append(row.astype(float))\n",
        "    \n",
        "    # return weights\n",
        "    return weights\n",
        "\n",
        "    \n",
        "\"\"\"\n",
        "    Transfer the learning\n",
        "\"\"\"\n",
        "def train_convex_data(model, data):\n",
        "    # load data\n",
        "    x_train = data[0]\n",
        "    y_train = data[1]\n",
        "    x_test = data[2]\n",
        "    x_test = data[3]\n",
        "\n",
        "    # load weights for transfer learning\n",
        "    print(\"transferring learning & retraining...\")\n",
        "    transfer_weights = load_weights(\"mnist_weights.csv\")\n",
        "    model.set_weights(transfer_weights)\n",
        "\n",
        "    # train on new data\n",
        "    print(\"training on pre-processed student handwriting data...\")\n",
        "    model.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "t3g5Duu_C8uF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy Handwriting Model #\n",
        "Here's where the utility defined above is run in the order the pipeline requires. This is essentially the high-level pipeline deployment of the CNN."
      ],
      "metadata": {
        "id": "Y9tjoKmmDtNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train EMNIST ##\n",
        "Here, we train the model on the EMNIST dataset for transfer onto our own convex hull data. The size of EMNIST, the lack of data from our own testing, the similarity in contexts between the datasets, and the necessity for accurate readings all drive the need for transfer learning.\n",
        "\n",
        "We will essentially train a model on the EMNIST dataset, store the resulting weights in the CNN framework, then load those weights for later training/specialization on our own datasets.\n",
        "\n",
        "At a certain stage in development, transfer learning will become obsolete in this use-case since student data will far surpass the amount of EMNIST data, enabling us to relinquish this dependency and specialize, perhaps offering performance improvements."
      ],
      "metadata": {
        "id": "IhMmu_A9D6gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" ::: STARTED MODEL TRAINING ::: \")\n",
        "\n",
        "# load data #\n",
        "emnist_data = load_emnist()\n",
        "\n",
        "# train model for transfer #\n",
        "handwriting_model = model_architecture()\n",
        "handwriting_model = train_emnist(handwriting_model, emnist_data)\n",
        "\n",
        "# save weights #\n",
        "save_weights(handwriting_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "bRyicZ01Ew1B",
        "outputId": "30ae983a-203e-4489-967c-6111e8a8eb4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ::: STARTED MODEL TRAINING ::: \n",
            "loading data for transfer learning...\n",
            "defining model architecture...\n",
            "compiling model...\n",
            "training on MNIST data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-20dac4a5fe8b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# train model for transfer #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhandwriting_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhandwriting_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_emnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandwriting_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memnist_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# save weights #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-11e3aca60f6b>\u001b[0m in \u001b[0;36mtrain_emnist\u001b[0;34m(model, emnist_data)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# train model on MNIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training on MNIST data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# save weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1850\u001b[0m             )\n\u001b[1;32m   1851\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 112800\n  y sizes: 18800\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train on Student Handwriting ##\n",
        "Here, we train the model on the convex hull data we have produced."
      ],
      "metadata": {
        "id": "1P3rYRLIF1Mx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4Utf-jOGHhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    Model Testing :: ChatGPT produced model testing example\n",
        "\"\"\"\n",
        "example_index = 0\n",
        "example_image = x_test[example_index]\n",
        "example_label = y_test[example_index]\n",
        "\n",
        "# Reshape the image to match the input shape of the model\n",
        "example_image = example_image.reshape(1, 28, 28, 1)\n",
        "\n",
        "# Make a prediction\n",
        "prediction = model.predict(example_image)\n",
        "\n",
        "# Get the predicted label (the index with the highest probability)\n",
        "predicted_label = tf.argmax(prediction, axis=1)\n",
        "\n",
        "print(\"Example:\")\n",
        "print(\"True Label:\", tf.argmax(example_label))\n",
        "print(\"Predicted Label:\", predicted_label.numpy()[0])\n"
      ],
      "metadata": {
        "id": "ymePh4Z_D385"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}